{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsl4WA5LbjVe"
   },
   "source": [
    "# Naina Chabra\n",
    "# 210968234\n",
    "# week 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5iXMEACb2hX"
   },
   "source": [
    "Gym is a standard API for reinforcement learning, and a diverse collection of reference environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKU2iY_7at8Y",
    "outputId": "5bb9870f-7527-438b-eafb-cd4272c0404c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1OYR9DEmGvg0"
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK8mt0fLOGBz"
   },
   "source": [
    "# **Cartpole Environment**\n",
    "\n",
    ">  A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "**Action Space**:Discrete(2)\n",
    "\n",
    "**Observation Shape**:(4,)\n",
    "\n",
    "**Observation High**:[4.8 inf 0.42 inf]\n",
    "\n",
    "**Observation Low**:[-4.8 -inf -0.42 -inf]\n",
    "\n",
    "**Import**:gym.make(\"CartPole-v1\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnVtR3trNpY7"
   },
   "source": [
    "Implement the CartPole environment for a certain number of steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPmm9cIRhmPM"
   },
   "source": [
    "**What is an Environment**\n",
    "\n",
    "In reinforcement learning, the environment is the surroundings or situations in which the agent is performing its actions and realizing the reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxC5amyFfHav"
   },
   "source": [
    "**What is a step**\n",
    "\n",
    "In reinforcement learning, the state is the current situation of the environment and provides useful information to know how to evaluate the right action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MC4vU12cGxND"
   },
   "outputs": [],
   "source": [
    "def run_for_steps(steps=100):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    total_reward = 0\n",
    "    for step in range(steps):\n",
    "      observation = env.reset()\n",
    "      action = env.action_space.sample() # take a random action\n",
    "      observation, reward, done, info = env.step(action)\n",
    "      total_reward += reward\n",
    "      if done:\n",
    "          break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK2_KW-1bghq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ePdbQSbNq_b"
   },
   "source": [
    "Implement the CartPole environment for a certain number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK2zvvx7efGL"
   },
   "source": [
    "**What is An Episode**\n",
    "\n",
    "In reinforcement learning, the episode is the recording of actions and states that an agent performed from a start state to an end state\n",
    "\n",
    "**When does An Episode End**\n",
    "\n",
    "The episode ends if any one of the following occurs:\n",
    "\n",
    "Termination: Pole Angle is greater than ±12°\n",
    "\n",
    "Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "\n",
    "Truncation: Episode length is greater than 500 (200 for v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "u71gabw2Gz0j"
   },
   "outputs": [],
   "source": [
    "def run_for_episodes(episodes=100):\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    total_reward = 0\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            action = env.action_space.sample() # take a random action\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        total_reward += episode_reward\n",
    "    return total_reward / episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lXw0yhRNt6r"
   },
   "source": [
    "### Compare and comment on the rewards earned for both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebBTEr0OeSXY"
   },
   "source": [
    " **What is a Reward**\n",
    "\n",
    " the reward is the feedback provided to an agent when a specific action is taken at a particular state in the environment.\n",
    "\n",
    " **What is considered As A Reward in the Cartpole Environment**\n",
    "\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 475 for v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7S8e3N1DG2a7",
    "outputId": "25b34bec-fb0b-45e3-b626-6722e0c29dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for running for steps: 100.0\n",
      "Reward for running for episodes: 21.45\n"
     ]
    }
   ],
   "source": [
    "steps_reward = run_for_steps()\n",
    "episodes_reward = run_for_episodes()\n",
    "print(f\"Reward for running for steps: {steps_reward}\")\n",
    "print(f\"Reward for running for episodes: {episodes_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRh0H2acQqxC"
   },
   "source": [
    "Reward for running for 500 steps: 500.0\n",
    "\n",
    "Reward for running for 500 episodes: 21.798"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUa6KDwmQSKz"
   },
   "source": [
    "Reward for running for 1000 steps: 1000.0\n",
    "\n",
    "Reward for running for 1000 episodes: 22.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsrK8yGuKVB8"
   },
   "source": [
    "Given that the environment is reset after each episode but continues for a given amount of steps, the prizes obtained for each strategy will probably change. In addition, the computation of the reward for the number of steps is cumulative, whereas the calculation for the number of episodes is averaged. Hence the reward for running episodes lie within close proximity even after changing the no. of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_Jxy3fj-RSn6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
